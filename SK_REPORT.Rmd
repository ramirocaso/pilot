---
title: "Pilot Study Results"
author: "Ramiro Casó - Rotterdam School of Management"
date: "`r Sys.Date()`"
output:
  html_document:
    css: custom.css
    theme: journal
    toc: yes
    toc_float: yes
    number_sections: yes
  word_document:
    toc: yes
    fig_caption: yes # Example of a Word-specific option
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE
)
```


```{r libraries}
library(kableExtra)
library(knitr)
library(tidyverse) # Includes ggplot2, dplyr, and other data manipulation libraries
library(readr)
library(rstatix)
library(ggpubr)
library(emmeans)
library(lmtest)
library(tidyr)
library(RColorBrewer)
```

# Intro

Hello Bram, Steven, how are you?

I've just wrapped up the analysis on the subjective knowledge scale, and I wanted to share something that I think might be interesting. At the end of this report are the details, but as an intro, here is a summary: 

**The Analysis:**

In line with our last meeting, I've implemented the controls you suggested, such as subtracting the values for Time 1 (T1) and using them as a control variable. I also combined both batches of participants—with and without visual aid—and incorporated them as another control variable.

**Main Findings:**

Here's the main finding so far:

- Understanding of Metabolism: Regardless of the text type, participants seemed to feel they understood metabolism better after reading explanatory text. This is seen in the rise in perceived understanding from T1 to T2 then its reversal after objective knowledge testing at T3.
- Metaphors Matter: An extended apt metaphor seemed to have a slight but noticeable effect compared to a non-apt metaphor. This could indicate that metaphors enhance subjective knowledge, and the aptness of the metaphor might further amplify this effect.

**Thoughts and Recommendations:**

I think our Objective Knowledge measure might need a revamp. While it did have a moderate effect, I feel it didn't quite shatter the illusion of knowing as we hope it would. Perhaps asking participants to explain what they just read could yield a stronger effect, similar to what Fernbach does in his studies. If apt metaphors indeed increase subjective knowledge, participants in that condition may resist this illusion-shattering more robustly.

I hope these insights make sense. Obviously,your thoughts and suggestions on the analysis below are more than welcome! 

**Moving Forward:**

I'll continue to analyse the remaining variables, but I wanted to get this to you quickly, as we might still have time to gather new data.

That's all for now. I'm looking forward to hearing your thoughts.

¡Saludos!

Ramiro

**p.s:** You probably already noticed it, but the menu in the top left side of the page helps you navigate through the report. It seemed like a lot of work to make the report, but it wasn't, and I believe it could be a better way to share results with you hereafter. So also, let me know what you think about this "new method" of reporting ;)  

------------------------------------------------------------------------

```{r loading the data}
exp_data <- read_csv("study2b_total.csv")
exp_data[, c(46, 51, 56)] <- lapply(exp_data[, c(46, 51, 56)], as.factor)
```

# Data Cleaning

First, and just like you suggested, I'm using the two batches together and included a new variable called `pilot`, which differentiates the first batch (pilot_a = with visual aid) from the second batch (pilot_b = without visual aid)

Second, I'm removing participants that either answered too fast (below 5 minutes) or took too long (above 25 minutes).

Here is a histogram of `duration_min` along with the summary statistics.

---

```{r cleaning}

ggplot(exp_data, aes(x = duration_min)) +
  geom_histogram(binwidth = 2, fill = "blue", alpha = 0.7) +
  labs(title = "Duration in Min",
       x = "Duration (Minutes)",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title


# Calculate the summary statistics
duration_summary <- summary(exp_data$duration_min)

# Create a data frame to hold the summary
summary_df <- data.frame(
  Statistic = c("Minimum", "1st Quartile", "Median", "Mean", "3rd Quartile", "Maximum"),
  Value = c(duration_summary[1], duration_summary[2], duration_summary[3], mean(exp_data$duration_min), duration_summary[5], duration_summary[6])
)

# Round the values to 2 decimal points
summary_df$Value <- round(summary_df$Value, 2)

kable(summary_df, format = "pandoc", caption = "Summary of Duration (Minutes)", align = c("c", "c"), row.names = FALSE)


```

---

As can be observed, a few observations clearly did not take the survey seriously. I'm trimming all participants whose duration is below 4 minutes and above 20 minutes.

Here are the resulting histogram and descriptives of `duration_min` after the removal of observations. 

---

```{r triming}


# Calculate the 5th and 95th percentiles of duration_min
#lower_bound <- quantile(exp_data$duration_min, 0.10)
#upper_bound <- quantile(exp_data$duration_min, 0.90)

lower_bound <- 4  # Lower bound set to 4 minutes
upper_bound <- 20 # Upper bound set to 20 minutes


# Filter the data to retain only observations between the 5th and 95th percentiles
exp_data_t <- exp_data[exp_data$duration_min > lower_bound & exp_data$duration_min < upper_bound, ]

#exp_data_t <- exp_data[exp_data$duration_min > lower_bound & exp_data$duration_min < upper_bound & exp_data$pilot == "pilot_b", ]

ggplot(exp_data_t, aes(x = duration_min)) +
  geom_histogram(binwidth = 2, fill = "blue", alpha = 0.7) +
  labs(title = "Duration in Min",
       x = "Duration (Minutes)",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title

# Calculate the summary statistics
duration_summary <- summary(exp_data_t$duration_min)

# Create a data frame to hold the summary
summary_df <- data.frame(
  Statistic = c("Minimum", "1st Quartile", "Median", "Mean", "3rd Quartile", "Maximum"),
  Value = c(duration_summary[1], duration_summary[2], duration_summary[3], mean(exp_data_t$duration_min), duration_summary[5], duration_summary[6])
)

# Round the values to 2 decimal points
summary_df$Value <- round(summary_df$Value, 2)

kable(summary_df, format = "pandoc", caption = "Summary of Duration (Minutes)", align = c("c", "c"), row.names = FALSE)



```

---

The new minimum duration is `r round(min(exp_data_t$duration_min), 2)`

The new maximum duration is `r round(max(exp_data_t$duration_min), 2)`

This reduces the data set to a total of `r nrow(exp_data_t)` observations. 

---

# Descriptives


With the clean data, here is a table with the mean, standard deviation, and number of observations of the main variables. . 

For clarity, these are the names of the variables.

- `text_type`: The main independent variable indicating the type of text that was presented, which has three levels:
  - `met_a` = metaphor apt;
  - `met_na` = non-apt metaphor;
  - `literal` = control condition.
- `ave_sk_t1`: Mean Subjective Knowledge before presenting the Text (Time 1).
- `ave_sk_t2`: Mean Subjective Knowledge after presenting the Text (Time 2).
- `ave_sk_t3`: Mean Subjective Knowledge after presenting the Objective Knowledge Scale (Time 3).
- `ave_wta`: Average of the Willingness to Adopt scale.
- `total_ok`: Results of Objective Knowledge Scale calculated by adding the correct
- `duration_min`: Average time it took to complete the survey in minutes.

---

```{r descriptives}

# Compute means and standard errors

variables <- c("ave_sk_t1", "ave_sk_t2", "ave_sk_t3","total_ok","ave_wta", "duration_min")

data_summary <- exp_data_t %>%
  gather(variable, value, all_of(variables)) %>% # Using all_of for variable selection
  group_by(variable) %>% # Grouping by variable only
  summarise(
    mean = mean(value, na.rm = TRUE),
    se = sd(value, na.rm = TRUE) / sqrt(n()),
    p10 = quantile(value, 0.10, na.rm = TRUE), # 10th percentile
    p25 = quantile(value, 0.25, na.rm = TRUE), # 25th percentile
    p50 = quantile(value, 0.50, na.rm = TRUE), # 50th percentile (median)
    p75 = quantile(value, 0.75, na.rm = TRUE), # 75th percentile
    n = n()
  )

data_summary_grouped <- exp_data_t %>%
  gather(variable, value, all_of(variables)) %>% # Using all_of for variable selection
  group_by(text_type, variable) %>%
  summarise(
    mean = mean(value, na.rm = TRUE),
    se = sd(value, na.rm = TRUE) / sqrt(n()),
    p10 = quantile(value, 0.10, na.rm = TRUE), # 10th percentile
    p25 = quantile(value, 0.25, na.rm = TRUE), # 25th percentile
    p50 = quantile(value, 0.50, na.rm = TRUE), # 50th percentile (median)
    p75 = quantile(value, 0.75, na.rm = TRUE), # 75th percentile
    n = n()
  )

## First table

data_summary %>%
  kable(format = "pandoc", digits = 2, align = 'c', caption = "Summary Statistics for All Variables") %>%
  kable_styling(full_width = FALSE)

# Creating a table for data_summary_grouped
data_summary_grouped %>%
  kable(format = "pandoc", digits = 2, align = 'c', caption = "Summary Statistics for Groped Variables") %>%
  kable_styling(full_width = FALSE)


```

After the trim, we end up with more than 30 observations per condition. This, however, will change ahead, since the the variable `pilot` which I used to differentiate the two badges **did have an effect** on some of the analysis

The following sections explore each variable in detail. 

--- 

# Subjective Knowledge

Let's analyze it, starting with the subjective knowledge scale.

Again, for clarity, keep in mind that:

- `ave_sk_t1`: Mean Subjective Knowledge before presenting the Text (Time 1).
- `ave_sk_t2`: Mean Subjective Knowledge after presenting the Text (Time 2).
- `ave_sk_t3`: Mean Subjective Knowledge after presenting the Objective Knowledge Scale (Time 3).
- `delta_sk2_sk1`: The difference between `ave_sk_t2` and `ave_sk_t1`
- `delta_sk3_sk2`: The difference between `ave_sk_t3` and `ave_sk_t2`
- `delta_sk3_sk1`: The difference between `ave_sk_t3` and `ave_sk_t1`

## SK ANOVA's

First, let's run a simple Repeated measured ANOVA. I'm including the variable `pilot`as a covariate, as you can see in the code below.

---

```{r sk dataframes}

sk_df <- exp_data_t %>%
  select(ResponseId, ave_sk_t1, ave_sk_t2, ave_sk_t3, text_type, pilot, ave_time) %>%
  pivot_longer(
    cols = c(ave_sk_t1, ave_sk_t2, ave_sk_t3),
    names_to = "sk_time",
    values_to = "ave_sk"
  )

sk_df2 <- exp_data_t %>%
  select(ResponseId, text_type,ave_sk_t1, ave_sk_t2, ave_sk_t3, delta_sk2_sk1, delta_sk3_sk2, delta_sk3_sk1, pilot, ave_time, total_ok) %>%
  mutate(delta_sk2_sk1_sq = delta_sk2_sk1^2,
         delta_sk3_sk2_sq = delta_sk3_sk2^2,
         delta_sk3_sk1_sq = delta_sk3_sk1^2)

sk_df3 <- exp_data_t %>%
  select(ResponseId, ave_sk_t1, ave_sk_t2, ave_sk_t3, text_type, ave_time, pilot, total_ok) 
```

```{r sk anova with control, echo=TRUE}

# Conduct the repeated measures ANOVA
anova_sk <- aov(ave_sk ~ sk_time + pilot + ave_time + Error(ResponseId/sk_time), data = sk_df)
#anova_sk <- aov(ave_sk ~ sk_time + Error(ResponseId/sk_time), data = sk_df)


# Print the summary
summary(anova_sk)

```

If I'm reading the output correctly, `pilot` didn't have an effect on Subjective Knowledge. In that sense, we can keep the whole data set.

Below we can see a table with the means per condition, a line graph that show the results and the post-hoc analysis 

---

```{r sk ANOVA}

# Calculate the means and standard errors
sk_summary <- sk_df %>%
  group_by(sk_time) %>%
  summarise(
    n_obs = n(),
    mean_ave_sk = mean(ave_sk, na.rm = TRUE),
    se_ave_sk = sd(ave_sk, na.rm = TRUE) / sqrt(n()),
    .groups = "drop" # This line avoids a warning message
  )

# Formatting the table with kable and kableExtra
sk_summary %>%
  kable("pandoc") %>%
  kable_styling("striped", full_width = FALSE) %>%
  add_header_above(c("Time Point" = 1, "Number of Observations" = 1, "Subjective Knowledge" = 2)) %>%
  column_spec(2:3, bold = TRUE, color = "blue")


# Create the plot
ggplot(sk_summary, aes(x=sk_time, y=mean_ave_sk, group=1)) +
  geom_line(color="blue") + # Line connecting the points
  geom_point(size=3, color="red") + # Points for the means
  geom_errorbar(aes(ymin=mean_ave_sk - se_ave_sk, ymax=mean_ave_sk + se_ave_sk), width=0.2) + # Error bars
  labs(
    title="Effect of SK Time on Average SK",
    x="SK Time",
    y="Average SK"
  ) +
  ylim(1, 7) + # Set the limits of the Y-axis
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title


# Obtain the estimated marginal means
emm <- emmeans(anova_sk, ~ sk_time)

# Perform pairwise comparisons with Tukey adjustments
pairs <- pairs(emm, adjust = "tukey")

# Print the results
print(pairs)

```

Looking at the plot, it is clear (as we saw) that the differences are due to the lower scores in the first measure of SK. 

It seems that regardless of the experimental text, participants increased their SK after being presented with the text. There is a slight decrease in SK for time 3, that goes from 5.28 to 5.06, which seems to be moderately significant, according to the post-hoc analysis. 

In other words, without accounting for the experimental stimuli, participants first increased their **Subjective Knowledge** after being presented with the text and decrease it after being asked to complete the **Objective Knowledge** scale. 

Next, I conducted a mixed two-way ANOVA

---

## Mixed two-way ANOVA 

```{r sk mixed anova}
# Load the necessary libraries
library(nlme)
library(car) # For Type III SS

# Ensure that sk_time and text_type are factors
sk_df$sk_time <- factor(sk_df$sk_time)
sk_df$text_type <- factor(sk_df$text_type)
sk_df <- na.omit(sk_df)

# Fit the model with sk_time as a within-subjects factor, text_type as a between-subjects factor, and a random intercept for each subject

#model <- lme(ave_sk ~ text_type * sk_time, random = ~1 | ResponseId, data = sk_df)

model <- lme(ave_sk ~ text_type * sk_time + pilot + ave_time, random = ~1 | ResponseId, data = sk_df)

# Use the Anova function from the car package to get Type III SS
mix_anova_sk <- Anova(model, type = "III")

# Print the result
print(mix_anova_sk)

```

the results indicate that both `text_type` and `sk_time` have significant main effects on the response variable `ave_sk`, but their interaction is not significant. Again, the `pilot` variable does not have a significant effect.

Below are the table with the means, the plot and the posthoc test for the `text_type` and `sk_time` 
For ease of reading, I'm going to present the posthoc comparisons separately. 
---

```{r sk mixed anova visualization}
# Calculate the means and standard errors for each combination of text_type and sk_time

sk_summary <- sk_df %>%
  group_by(text_type, sk_time) %>%
  summarise(
    n_obs = n(),
    mean_ave_sk = mean(ave_sk, na.rm = TRUE),
    se_ave_sk = sd(ave_sk, na.rm = TRUE) / sqrt(n()),
    .groups = "drop")

# Formatting the table with kable and kableExtra
sk_summary %>%
  kable("pandoc") %>%
  kable_styling("striped", full_width = FALSE) %>%
  add_header_above(c(" " = 2, "Subjective Knowledge" = 3)) %>% # Corrected the number of columns
  column_spec(3:5, bold = TRUE, color = "blue") # Adjusted the column range

# Create the plot
ggplot(sk_summary, aes(x=sk_time, y=mean_ave_sk, group=text_type, color=text_type)) +
  geom_line(linewidth=1) + # Lines connecting the points
  geom_point(linewidth=3) + # Points for the means
  geom_errorbar(aes(ymin=mean_ave_sk - se_ave_sk, ymax=mean_ave_sk + se_ave_sk), width=0.2) + # Error bars
  labs(
    title="Effect of SK Time and Text Type on Average SK",
    x="SK Time",
    y="Average SK"
  ) +
  ylim(1, 7) + # Set the limits of the Y-axis
  theme_minimal() +
  scale_color_brewer(palette="Set1") + # Use a color palette for the different text types 
  theme(plot.title = element_text(hjust = 0.5)) # Center the title

```

---

<center>**Pairwise comparison just for text_type **.</center>

```{r}
emm_text_type <- emmeans(model, ~ text_type)
pairs_text_type <- pairs(emm_text_type, adjust = "tukey")
summary_pairs_text_type <- summary(pairs_text_type)
print(summary_pairs_text_type)
```


---

<center>**Pairwise comparison just for sk_time **.</center>


```{r}
emm_sk <- emmeans(model, ~ sk_time)
pairs_sk <- pairs(emm_sk, adjust = "tukey")
summary_pairs_sk <- summary(pairs_sk)
print(summary_pairs_sk)
```


These results, to me, are indicative that the stimuli are not working as intended. 

When we look at the effect of `sk_time` on `ave_sk`, it seems that presenting participants with an explicative text does not lead to the shattering of the illusion of knowing. Perhaps this is because participants aren't required to make any effort to explain how metabolism works but instead are **"passively"** consuming information that differs in format. In that sense, it is possible that participants feel that they have **"learned"** something, increasing their rating `ave_sk` instead of realizing they don't know much about the subject. 

This effect of `sk_time` on `ave_sk` is shown both in the highly significant coefficient as well as in the posthoc analysis. In the latter, all of the instances present significant difference, with the difference between T2 and T3 slightly less strong (p < 0.1). 

What this indicates is that indeed participants "gained" subjective knowledge when presented with the stimuli and then lost it after they were presented with the Objective Knowledge Scale. 

**This is VERY IMPORTANT (I believe)**

When we look at the effect of `text_type` on `ave_sk`, it seems that participants in the apt metaphor condition report slightly higher levels  of subjective knowledge than participants in the non-apt metaphor condition, although this is not reflected in the posthoc analysis. 

Now I'm going to perform the analysis using the deltas. 

## SK Deltas

First, let's take a look at the distribution of the Deltas. 

```{r deltas vis}

# Reshape the data into a long format
sk_df2_long <- sk_df2 %>%
  select(delta_sk2_sk1, delta_sk3_sk2, delta_sk3_sk1, delta_sk2_sk1_sq, delta_sk3_sk2_sq, delta_sk3_sk1_sq) %>%
  gather(key = "variable", value = "value")

# Create the faceted plot
ggplot(sk_df2_long, aes(x = value)) +
  geom_histogram(binwidth = 0.5, fill = "blue", alpha = 0.7) +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  labs(
    title = "Histograms of Deltas of Subjective Knowledge",
    x = "Value",
    y = "Frequency"
  ) +
  theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5))

```

### SK Deltas

First, I want to know if there are differences in `delta_sk2_sk1`

```{r}
sk_df2 <- na.omit(sk_df2)
model_delta <- aov(delta_sk2_sk1 ~ text_type + pilot + ave_time, data = sk_df2)
#model_delta <- aov(delta_sk2_sk1 ~ text_type, data = sk_df2)

summary(model_delta)


```

Here is the plot.

```{r}

# Calculate the means and standard errors
means_se <- sk_df2 %>%
  group_by(text_type) %>%
  summarise(
    mean_delta = mean(delta_sk2_sk1, na.rm = TRUE),
    se_delta = sd(delta_sk2_sk1, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Create the plot
ggplot(means_se, aes(x = text_type, y = mean_delta, group = 1)) +
  geom_bar(stat = "identity", fill = "orange", color = "blue") + # Bar for the means
  geom_errorbar(aes(ymin = mean_delta - se_delta, ymax = mean_delta + se_delta), width = 0.2) + # Error bars
  labs(
    title = "Effect of Text Type on Delta SK2 SK1",
    x = "Text Type",
    y = "Mean Delta SK2 SK1"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title


```

---

Second, I want to know if there are differences in `delta_sk3_sk2`

```{r}
sk_df2 <- na.omit(sk_df2)
model_delta <- aov(delta_sk3_sk2 ~ text_type + pilot + ave_time, data = sk_df2)
#model_delta <- aov(delta_sk2_sk1 ~ text_type, data = sk_df2)

summary(model_delta)


```

Here is the plot.

```{r}

# Calculate the means and standard errors
means_se <- sk_df2 %>%
  group_by(text_type) %>%
  summarise(
    mean_delta = mean(delta_sk3_sk2, na.rm = TRUE),
    se_delta = sd(delta_sk3_sk2, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Create the plot
ggplot(means_se, aes(x = text_type, y = mean_delta, group = 1)) +
  geom_bar(stat = "identity", fill = "orange", color = "blue") + # Bar for the means
  geom_errorbar(aes(ymin = mean_delta - se_delta, ymax = mean_delta + se_delta), width = 0.2) + # Error bars
  labs(
    title = "Effect of Text Type on Delta SK3 SK2",
    x = "Text Type",
    y = "Mean Delta SK2 SK2"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title


```


```{r squared deltas 1, eval =FALSE, include=FALSE}

model_delta_sq <- aov(delta_sk2_sk1_sq ~ text_type + pilot, data = sk_df2)
#model_delta_sq <- aov(delta_sk2_sk1_sq ~ text_type, data = sk_df2)

summary(model_delta_sq)

```

```{r squared deltas 2, eval=FALSE, include=FALSE}

# Calculate the means and standard errors
means_se <- sk_df2 %>%
  group_by(text_type) %>%
  summarise(
    mean_delta = mean(delta_sk2_sk1_sq, na.rm = TRUE),
    se_delta = sd(delta_sk2_sk1_sq, na.rm = TRUE) / sqrt(n()),
    .groups = "drop"
  )

# Create the plot
ggplot(means_se, aes(x = text_type, y = mean_delta, group = 1)) +
  geom_line(color = "blue") + # Line connecting the points
  geom_point(size = 3, color = "red") + # Points for the means
  geom_errorbar(aes(ymin = mean_delta - se_delta, ymax = mean_delta + se_delta), width = 0.2) + # Error bars
  labs(
    title = "Effect of Text Type on Squared Delta SK2 SK1",
    x = "Text Type",
    y = "Mean Delta SK2 SK1"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) # Center the title

```

### ANOVA for av_sk_t2 controlling for av_sk_t1

Next, I want to see if there are differences in the average subjective knowledge reported in Time 2 (after the experimental stimuli was presented) controlling for the score of av_sk_t1 and pilot. 

```{r sk controlling a}
# clean missing values
sk_df3 <- na.omit(sk_df3)

# Fit the ANCOVA model
ancova_model_a <- lm(ave_sk_t2 ~ ave_sk_t1 + text_type + pilot, data = sk_df3)

# Summary of the model
summary(ancova_model_a)

```

Here are the plots

```{r sk controlling a2}



text_type_levels <- levels(sk_df3$text_type)
colors <- brewer.pal(length(text_type_levels), "Set1")

for (pilot_level in unique(sk_df3$pilot)) {
  subset_data <- sk_df3[sk_df3$pilot == pilot_level,]
  plot(NULL, xlim=c(0.5, length(text_type_levels) + 0.5), ylim=range(subset_data$ave_sk_t2),
       xlab="Text Type", ylab="Average Subjective Knowledge at Time 2",
       main=paste("Relationship between ave_sk_t2 and text_type (", pilot_level, ")", sep=""),
       xaxt="n")
  axis(1, at=1:length(text_type_levels), labels=text_type_levels)
  for (i in 1:length(text_type_levels)) {
    points(rep(i, nrow(subset_data[subset_data$text_type == text_type_levels[i],])), 
           subset_data$ave_sk_t2[subset_data$text_type == text_type_levels[i]], col=colors[i], pch=16)
  }
  legend("topright", legend=text_type_levels, col=colors, pch=16, title="Text Type")
  
  # Adding the fitted line from the ANCOVA model
  lines(1:length(text_type_levels), 
        sapply(text_type_levels, function(tt) {
          predict(ancova_model_a, newdata=data.frame(ave_sk_t1=mean(subset_data$ave_sk_t1), 
                                                   text_type=tt, pilot=pilot_level))
        }), col="red", lwd=2)
}

```

As you can see, The coefficients for the `text_type` factor levels "met_a" and "met_na" are 0.0983 and -0.0230, respectively. Neither of these coefficients is statistically significant (p > 0.05)
so there is no evidence of a difference in `ave_sk_t2` across the `text_type` levels, after controlling for `ave_sk_t1` and `pilot`.

There is a mildly significant effect of `pilot` -0.27348 (p < 0.1). 

Finally, `ave_sk_t1` has a coefficient of 0.4230 which is highly significant (p < 0.01), which makes sense since we saw that the scores of SK are related to the scores at time 2. 

### ANOVA for av_sk_t3 controlling for av_sk_t1

Lastly, I want to see if there are differences in the average subjective knowledge reported in Time 3 (after the experimental stimuli was presented) controlling for the score of av_sk_t1 and pilot. 

```{r}
# Remove NA values
sk_df3 <- na.omit(sk_df3)

# Fit the ANCOVA model
ancova_model_b <- lm(ave_sk_t3 ~ text_type + pilot + ave_sk_t1 + total_ok, data = sk_df3)

# Summary of the model
summary(ancova_model_b)

text_type_levels <- levels(sk_df3$text_type)
colors <- brewer.pal(length(text_type_levels), "Set1")

for (pilot_level in unique(sk_df3$pilot)) {
  subset_data <- sk_df3[sk_df3$pilot == pilot_level,]
  plot(NULL, xlim=c(0.5, length(text_type_levels) + 0.5), ylim=c(1, 7),
       xlab="Text Type", ylab="Average Subjective Knowledge at Time 3",
       main=paste("Relationship between ave_sk_t3 and text_type (", pilot_level, ")", sep=""),
       xaxt="n")
  axis(1, at=1:length(text_type_levels), labels=text_type_levels)
  for (i in 1:length(text_type_levels)) {
    points(rep(i, nrow(subset_data[subset_data$text_type == text_type_levels[i],])), 
           subset_data$ave_sk_t3[subset_data$text_type == text_type_levels[i]], col=colors[i], pch=16)
  }
  legend("topright", legend=text_type_levels, col=colors, pch=16, title="Text Type")
  
# Adding the fitted line from the ANCOVA model
  lines(1:length(text_type_levels), 
        sapply(text_type_levels, function(tt) {
          new_data <- data.frame(ave_sk_t1=mean(subset_data$ave_sk_t1), 
                                 text_type=tt, pilot=pilot_level,
                                 total_ok=mean(subset_data$total_ok)) # Include total_ok
          predict(ancova_model_b, newdata=new_data)
        }), col="red", lwd=2)

}

```

---

In the case of `ave_sk_t3` `ave_sk_t1`and `total_ok` show significant effects. The variable pilot may have a marginal effect, while the `text_type` variable does not appear to have a significant relationship with IV. 

Below I included `total_ok`as an IV just to see if there was any significant interaction, but I didn't find one. 

```{r}
ancova_model_c <- lm(ave_sk_t3 ~ text_type * total_ok + pilot + ave_sk_t1, data = sk_df3)
summary(ancova_model_c)


# Create a grid of total_ok values for prediction
new_data <- expand.grid(
  text_type = levels(sk_df3$text_type),
  total_ok = seq(min(sk_df3$total_ok), max(sk_df3$total_ok), length.out = 100),
  pilot = unique(sk_df3$pilot), # Include other variables as needed
  ave_sk_t1 = mean(sk_df3$ave_sk_t1) # Example: using the mean for ave_sk_t1
)

# Predict ave_sk_t3 using the new data
new_data$predicted <- predict(ancova_model_c, newdata = new_data)

# Plot the interaction
ggplot(new_data, aes(x = total_ok, y = predicted, color = text_type)) +
  geom_line() +
  labs(
    title = "Interaction between Text Type and Total OK",
    x = "Total OK",
    y = "Predicted Average Subjective Knowledge at Time 3"
  ) +
  theme_minimal() +
  facet_wrap(~ pilot) # Separate plots for each level of pilot, if desired

```


---

## Discussion of SK Scale. 

Just like before, I believe the stimuli are not working as predicted. 
Presenting participants with explanatory text, regardless of the type of text, seems to increase participants' perceived understanding of how metabolism work. 

When you think about it, this makes sense. 

Participants have no reason to feel their SK threatened in any way because their knowledge has not been tested yet. They probably think they know more since they have just acquired additional information. This effect is shown in the significant differences in SK between T1 and T2. Also, and perhaps more importantly, the difference between T2 and T3 is also significant, but in the opposite direction, meaning that after participants were asked to answer the Objective Knowledge Scale, their perceived understanding suffered. 

Finally, when the information is presented as an extended apt metaphor, it appears to have a more significant effect on SK than when presented with a non-apt extended metaphor. This effect is mild, but it could indicate that apt metaphors are better at increasing SK than non-apt metaphors.

